{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapley Values in Linear SEMs\n",
    "In the paper Budhathoki et. al.: Causal structure-based root cause analysis of outliers, Shapley values are used for root cause analysis. Our first question is to write out the formula for the shapley values in the case of a linear SEM. For $f$ the function of the SEM, $\\tau$ used in the outleier score (e.g. identity or $\\| \\cdot \\|$ or $\\tau(x) = |x-\\mu|$, which gives the z-score), $g=\\tau \\circ f$, the contribution of $j\\mid \\mathcal{I}$ is defined as\n",
    "$$C(j\\mid \\mathcal{I}) = -\\log P^{\\text{rd}(N_{\\mathcal{I} \\cup \\{j\\}})} \\{g(N) \\geq g(n)\\}+\\log P^{\\text{rd}(N_{\\mathcal{I}})} \\{g(N) \\geq g(n)\\}.$$\n",
    "The Shapley value is\n",
    "$$\\phi(j)=\\frac{1}{n!}\\sum_{\\sigma}C(j\\mid \\mathcal{I}_{j}^{\\text{prec};\\sigma})=\\sum_{\\mathcal{I}\\subseteq U\\setminus\\{j\\}}\\frac{1}{n \\binom{n-1}{|\\mathcal{I}|}}C(j\\mid \\mathcal{I}).$$\n",
    "With this setup, there is no simple formula for the Shapley value in linear SEMs, in contrast to some other papers that do not use the log transformation in their definition of contribution. \n",
    "\n",
    "### Comparison: Shapley values without log transformation\n",
    "For example, in \"Feature relevance quantification in explainable AI: A causal problem\" they want to attribute feature importance to each $x_j$ (of one sample x) in a model \n",
    "$$Y \\approx f(X_1, \\dots, X_n).$$\n",
    "Here, \n",
    "$$C(j\\mid T) = \\mathbb{E}(f(x_{T \\cup \\{j\\}}, X_{(T \\cup \\{j\\})^C})) - \\mathbb{E}(f(x_T, X_{T^C})).$$\n",
    "Hence, if $f(x) = a_1 x_1 + \\dots a_n x_n$ is linear, the contribution\n",
    "$$C(j\\mid T) =  \\mathbb{E}(f(x_{T \\cup \\{j\\}}, X_{(T \\cup \\{j\\})^C}) - f(x_T, X_{T^C})) = \\mathbb{E}(a_j x_j - a_j X_j) = a_j( x_j -\\mathbb{E}( X_j))$$\n",
    "does not depend on $T$.\n",
    "\n",
    "### Shapley values with log transformation\n",
    "Adding the log transformation, this simplification does not work anymore since\n",
    "$$C(j\\mid \\mathcal{I}) = -\\log P^{\\text{rd}(N_{\\mathcal{I} \\cup \\{j\\}})} \\{g(N) \\geq g(n)\\}+\\log P^{\\text{rd}(N_{\\mathcal{I}})} \\{g(N) \\geq g(n)\\} = \\log\n",
    "\\frac{ P^{\\text{rd}(N_{\\mathcal{I}})} \\{g(N) \\geq g(n)\\}}{P^{\\text{rd}(N_{\\mathcal{I} \\cup \\{j\\}})} \\{g(N) \\geq g(n)\\}}.$$\n",
    "and even if $f$ is linear, there is no easy way to get rid of this fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniela/anaconda3/envs/causal_reading/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fitting causal mechanism of node W: 100%|██████████| 4/4 [00:00<00:00, 150.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, networkx as nx\n",
    "from dowhy import gcm\n",
    "\n",
    "X = np.random.uniform(low=-5, high=5, size=1000)\n",
    "Y = 0.5 * X + np.random.normal(loc=0, scale=1, size=1000)\n",
    "Z = 2 * Y + np.random.normal(loc=0, scale=1, size=1000)\n",
    "W = 3 * Z + np.random.normal(loc=0, scale=1, size=1000)\n",
    "data = pd.DataFrame(data=dict(X=X, Y=Y, Z=Z, W=W))\n",
    "\n",
    "causal_model = gcm.InvertibleStructuralCausalModel(nx.DiGraph([('X', 'Y'), ('Y', 'Z'), ('Z', 'W')]))  # X -> Y -> Z -> W\n",
    "gcm.auto.assign_causal_mechanisms(causal_model, data)\n",
    "gcm.fit(causal_model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate set function: 16it [00:00, 52.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': array([0.28073264]),\n",
       " 'Y': array([7.30187141]),\n",
       " 'Z': array([0.11284866]),\n",
       " 'W': array([0.31124813])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.uniform(low=-5, high=5)  # Sample from its normal distribution.\n",
    "Y = 0.5 * X + 5  # Here, we set the noise of Y to 5, which is unusually high.\n",
    "Z = 2 * Y\n",
    "W = 3 * Z\n",
    "anomalous_data = pd.DataFrame(data=dict(X=[X], Y=[Y], Z=[Z], W=[W]))  # This data frame consist of only one sample here.\n",
    "\n",
    "attribution_scores = gcm.attribute_anomalies(causal_model, 'W', anomaly_samples=anomalous_data)\n",
    "attribution_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Compare results to the scores I would get when using the Shapley values as defined in \"Feature relevance quantification in explainable AI: A causal problem\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from dowhy.gcm.util.general import variance_of_deviations\n",
    "from dowhy.gcm.util.general import means_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate set function: 8it [00:00, 62.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -0.50471089,   2.00625383, 120.95873524])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.uniform(low=-5, high=5, size=1000)\n",
    "Y = 0.5 * X + np.random.normal(loc=0, scale=1, size=1000)\n",
    "Z = 2 * Y + np.random.normal(loc=0, scale=1, size=1000)\n",
    "W = 3 * Z + np.random.normal(loc=0, scale=1, size=1000)\n",
    "data = pd.DataFrame(data=dict(X=X, Y=Y, Z=Z, W=W))\n",
    "# add anomalous row\n",
    "X_outlier = np.random.uniform(low=-5, high=5)  # Sample from its normal distribution.\n",
    "Y_outlier = 0.5 * X_outlier + 5  # Here, we set the noise of Y to 5, which is unusually high.\n",
    "Z_outlier = 2 * Y_outlier\n",
    "W_outlier = 3 * Z_outlier\n",
    "W = np.append(W, W_outlier)\n",
    "anomalous_data = pd.DataFrame(data=dict(X=[X_outlier], Y=[Y_outlier], Z=[Z_outlier], W=[W_outlier]))  # This data frame consist of only one sample here.\n",
    "data = pd.concat([data, anomalous_data], ignore_index=True)\n",
    "\n",
    "mdl = LinearRegression()\n",
    "mdl.fit(data[['X', 'Y', 'Z']].to_numpy(), W)\n",
    "relevance = gcm.feature_relevance_distribution(mdl.predict, data[['X', 'Y', 'Z']].to_numpy(), subset_scoring_func=variance_of_deviations)\n",
    "relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate set function: 8it [00:00, 135.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.66288235e-02,  5.65014022e-01,  3.18805726e+01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_observation = anomalous_data[['X', 'Y', 'Z']].to_numpy()\n",
    "relevance = gcm.feature_relevance_sample(mdl.predict, data[['X', 'Y', 'Z']].to_numpy(), baseline_samples=single_observation, subset_scoring_func=means_difference)\n",
    "relevance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_reading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
